# KumoRFM â€“ Data Pipeline

This repository contains a data pipeline that:
1. Downloads **private datasets from Google Drive**
2. Transforms them locally
3. Creates a smaller subset of the data
4. Uploads results back to Google Drive

The pipeline is designed to be run with **one command**.

---

## ğŸ“ Project Structure

```
KumoRFM-usecase/
â”‚
â”œâ”€ run_pipeline.py          # Main entry point (recommended)
â”œâ”€ drive_sync.py            # Download data from Google Drive
â”‚
â”œâ”€ src/
â”‚   â”œâ”€ transform_data.py    # original â†’ cleaned
â”‚   â”œâ”€ make_small.py        # cleaned â†’ cleaned_small
â”‚   â””â”€ drive_upload.py      # upload results back to Drive
â”‚
â”œâ”€ datasets/
â”‚   â”œâ”€ cleaned/
â”‚   â””â”€ cleaned_small/
â”‚
â”œâ”€ drive_cache/             # Local cache of Drive data (auto-created)
â”‚   â””â”€ original/
â”‚
â”œâ”€ secrets/
â”‚   â”œâ”€ client_secret.json
â”‚   â””â”€ token.json
â”‚
â”œâ”€ requirements.txt
â””â”€ README.md
```

---

## ğŸ” Google Drive Access

- Data is stored in **private Google Drive folders**
- Access is handled via **Google Drive API (OAuth)**
- Each user authenticates once with their own Google account
- No public links and no service accounts are used

---

## 1ï¸âƒ£ Setup (one-time)

### Create and activate a virtual environment

**Windows (PowerShell)**
```powershell
python -m venv .venv
.venv\Scripts\Activate.ps1
```

**macOS / Linux**
```bash
python3 -m venv .venv
source .venv/bin/activate
```

---

### Install dependencies

```bash
pip install -r requirements.txt
```

---

## 2ï¸âƒ£ Google Drive configuration (one-time per user)

1. Obtain `client_secret.json` from the project owner
2. Place it in:
   ```
   secrets/client_secret.json
   ```
3. Ensure your Google account email is added as a **Test User**
4. Set the Drive folder ID (from the Drive URL)

**Windows**
```powershell
$env:KUMORFM_DRIVE_FOLDER_ID="YOUR_DRIVE_FOLDER_ID"
```

**macOS / Linux**
```bash
export KUMORFM_DRIVE_FOLDER_ID="YOUR_DRIVE_FOLDER_ID"
```

â¡ï¸ The first run will open a browser window for Google login.

---

## 3ï¸âƒ£ Run the full pipeline (recommended)

```powershell
python run_pipeline.py
```

This will:
1. Download data from Google Drive â†’ `drive_cache/`
2. Transform original data â†’ `datasets/cleaned/`
3. Create a reduced dataset â†’ `datasets/cleaned_small/`
4. Upload results back to Google Drive

---

## 4ï¸âƒ£ Skipping steps (for faster iteration)

### Skip Drive download
```powershell
python run_pipeline.py --skip-sync
```

### Only regenerate `cleaned_small`
```powershell
python run_pipeline.py --skip-sync --skip-transform --skip-upload
```

### Only upload results
```powershell
python run_pipeline.py --skip-sync --skip-transform --skip-small
```

---

## 5ï¸âƒ£ Optional parameters

### Change the time window (months)
```powershell
python run_pipeline.py --months 1
```

### Limit number of buildings
```powershell
python run_pipeline.py --max-buildings 50
```

Use `0` to keep all buildings (default).

---

## 6ï¸âƒ£ Output locations

### Local
```
drive_cache/original/
datasets/cleaned/
datasets/cleaned_small/
```

### Google Drive
```
datasets/cleaned/
datasets/cleaned_small/
```

Uploads update existing files (no duplicates).

---

## 7ï¸âƒ£ Troubleshooting

**403 / Access blocked**
- Your email is not added as a Test User

**Pipeline runs everything again**
- Use `--skip-*` flags

**Transform step is slow**
- Large CSVs â†’ expected behavior
